#!/usr/bin/env python3
"""
ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ í’ˆì§ˆì„ ì²´í¬í•˜ê³  ì ìˆ˜ë¥¼ ì¶œë ¥
"""
import sys
import argparse
from pathlib import Path
from datetime import datetime, timedelta
from collections import Counter
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
SCRIPT_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / 'src'))

from src.database.db_manager import init_database, get_db
from src.database.models import CollectedText
from src.preprocessing.text_cleaner import TextCleaner
from src.preprocessing.deduplicator import Deduplicator


class DataQualityChecker:
    """
    ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í´ë˜ìŠ¤
    """
    
    def __init__(self, db_session):
        """
        ë°ì´í„° í’ˆì§ˆ ê²€ì¦ê¸° ì´ˆê¸°í™”
        
        Args:
            db_session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        """
        self.db = db_session
        self.text_cleaner = TextCleaner()
        self.deduplicator = Deduplicator()
    
    def check_all(self, keyword: str = None, hours: int = 24) -> dict:
        """
        ì „ì²´ ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        
        Args:
            keyword: í‚¤ì›Œë“œ í•„í„° (Noneì´ë©´ ì „ì²´)
            hours: ê²€ì¦í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„)
            
        Returns:
            í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ë°ì´í„° ì¡°íšŒ
        start_time = datetime.utcnow() - timedelta(hours=hours)
        query = self.db.query(CollectedText)
        
        if keyword:
            query = query.filter(CollectedText.keyword == keyword)
        
        query = query.filter(CollectedText.collected_at >= start_time)
        texts = query.all()
        
        if not texts:
            return {"error": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # ê° í•­ëª©ë³„ ê²€ì¦
        results = {
            "total_count": len(texts),
            "keyword": keyword or "ì „ì²´",
            "time_range_hours": hours,
            "checks": {}
        }
        
        # 1. í…ìŠ¤íŠ¸ í’ˆì§ˆ ì²´í¬
        results["checks"]["text_quality"] = self._check_text_quality(texts)
        
        # 2. íƒ€ì„ìŠ¤íƒ¬í”„ ì •ìƒ ì—¬ë¶€
        results["checks"]["timestamp"] = self._check_timestamp(texts)
        
        # 3. ì–¸ì–´ ê°ì§€
        results["checks"]["language"] = self._check_language(texts)
        
        # 4. ì¤‘ë³µ ëŒ“ê¸€ ì—¬ë¶€
        results["checks"]["duplicates"] = self._check_duplicates(texts)
        
        # 5. í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
        if keyword:
            results["checks"]["keyword_match"] = self._check_keyword_match(texts, keyword)
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        results["quality_score"] = self._calculate_quality_score(results["checks"])
        
        return results
    
    def _check_text_quality(self, texts: list) -> dict:
        """
        í…ìŠ¤íŠ¸ í’ˆì§ˆ ì²´í¬
        
        Args:
            texts: í…ìŠ¤íŠ¸ ê°ì²´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
        """
        total = len(texts)
        empty_count = 0
        emoji_count = 0
        url_count = 0
        whitespace_ratio_sum = 0
        
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+",
            flags=re.UNICODE
        )
        
        url_pattern = re.compile(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        )
        
        for text_obj in texts:
            text = text_obj.text or ""
            
            if not text.strip():
                empty_count += 1
                continue
            
            # ì´ëª¨ì§€ ì²´í¬
            if emoji_pattern.search(text):
                emoji_count += 1
            
            # URL ì²´í¬
            if url_pattern.search(text):
                url_count += 1
            
            # ê³µë°± ë¹„ìœ¨ ê³„ì‚°
            if len(text) > 0:
                whitespace_ratio = text.count(' ') / len(text)
                whitespace_ratio_sum += whitespace_ratio
        
        avg_whitespace_ratio = whitespace_ratio_sum / total if total > 0 else 0
        
        return {
            "empty_text_ratio": empty_count / total if total > 0 else 0,
            "emoji_text_ratio": emoji_count / total if total > 0 else 0,
            "url_text_ratio": url_count / total if total > 0 else 0,
            "avg_whitespace_ratio": avg_whitespace_ratio,
            "score": 1.0 - min(1.0, (empty_count + emoji_count * 0.3 + url_count * 0.2) / total)
        }
    
    def _check_timestamp(self, texts: list) -> dict:
        """
        íƒ€ì„ìŠ¤íƒ¬í”„ ì •ìƒ ì—¬ë¶€ ì²´í¬
        
        Args:
            texts: í…ìŠ¤íŠ¸ ê°ì²´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            íƒ€ì„ìŠ¤íƒ¬í”„ ê²€ì¦ ê²°ê³¼
        """
        total = len(texts)
        null_count = 0
        future_count = 0
        old_count = 0
        
        now = datetime.utcnow()
        one_year_ago = now - timedelta(days=365)
        
        for text_obj in texts:
            if not text_obj.collected_at:
                null_count += 1
                continue
            
            if text_obj.collected_at > now:
                future_count += 1
            
            if text_obj.collected_at < one_year_ago:
                old_count += 1
        
        return {
            "null_timestamp_ratio": null_count / total if total > 0 else 0,
            "future_timestamp_ratio": future_count / total if total > 0 else 0,
            "old_timestamp_ratio": old_count / total if total > 0 else 0,
            "score": 1.0 - min(1.0, (null_count + future_count + old_count) / total)
        }
    
    def _check_language(self, texts: list) -> dict:
        """
        ì–¸ì–´ ê°ì§€ ì²´í¬
        
        Args:
            texts: í…ìŠ¤íŠ¸ ê°ì²´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì–¸ì–´ ê²€ì¦ ê²°ê³¼
        """
        total = len(texts)
        language_counts = Counter()
        
        for text_obj in texts:
            lang = self.text_cleaner.detect_language(text_obj.text or "")
            language_counts[lang] += 1
        
        korean_ratio = language_counts.get("ko", 0) / total if total > 0 else 0
        
        return {
            "korean_ratio": korean_ratio,
            "english_ratio": language_counts.get("en", 0) / total if total > 0 else 0,
            "mixed_ratio": language_counts.get("mixed", 0) / total if total > 0 else 0,
            "language_distribution": dict(language_counts),
            "score": korean_ratio  # í•œêµ­ì–´ ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
        }
    
    def _check_duplicates(self, texts: list) -> dict:
        """
        ì¤‘ë³µ ëŒ“ê¸€ ì²´í¬
        
        Args:
            texts: í…ìŠ¤íŠ¸ ê°ì²´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¤‘ë³µ ê²€ì¦ ê²°ê³¼
        """
        # ì¤‘ë³µ ì œê±° ì „í›„ ë¹„êµ
        text_dicts = [
            {"text": text_obj.text, "id": text_obj.id}
            for text_obj in texts
        ]
        
        unique_texts = self.deduplicator.remove_duplicates(text_dicts, key_field="text")
        
        duplicate_count = len(text_dicts) - len(unique_texts)
        duplicate_ratio = duplicate_count / len(text_dicts) if text_dicts else 0
        
        return {
            "total_count": len(text_dicts),
            "unique_count": len(unique_texts),
            "duplicate_count": duplicate_count,
            "duplicate_ratio": duplicate_ratio,
            "score": 1.0 - duplicate_ratio
        }
    
    def _check_keyword_match(self, texts: list, keyword: str) -> dict:
        """
        í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ ì²´í¬
        
        Args:
            texts: í…ìŠ¤íŠ¸ ê°ì²´ ë¦¬ìŠ¤íŠ¸
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            
        Returns:
            í‚¤ì›Œë“œ ë§¤ì¹­ ê²€ì¦ ê²°ê³¼
        """
        total = len(texts)
        matched_count = 0
        
        keyword_lower = keyword.lower()
        
        for text_obj in texts:
            text_lower = (text_obj.text or "").lower()
            if keyword_lower in text_lower:
                matched_count += 1
        
        match_ratio = matched_count / total if total > 0 else 0
        
        return {
            "matched_count": matched_count,
            "match_ratio": match_ratio,
            "score": match_ratio
        }
    
    def _calculate_quality_score(self, checks: dict) -> float:
        """
        ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        
        Args:
            checks: ê° ê²€ì¦ í•­ëª© ê²°ê³¼
            
        Returns:
            ì „ì²´ í’ˆì§ˆ ì ìˆ˜ (0.0 ~ 1.0)
        """
        weights = {
            "text_quality": 0.25,
            "timestamp": 0.15,
            "language": 0.20,
            "duplicates": 0.20,
            "keyword_match": 0.20
        }
        
        total_score = 0.0
        total_weight = 0.0
        
        for check_name, weight in weights.items():
            if check_name in checks and "score" in checks[check_name]:
                total_score += checks[check_name]["score"] * weight
                total_weight += weight
        
        return total_score / total_weight if total_weight > 0 else 0.0
    
    def print_report(self, results: dict):
        """
        ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥
        
        Args:
            results: ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if "error" in results:
            print(f"âŒ {results['error']}")
            return
        
        print("=" * 70)
        print("ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë¦¬í¬íŠ¸")
        print("=" * 70)
        print(f"\nğŸ“Š ê¸°ë³¸ ì •ë³´")
        print(f"  í‚¤ì›Œë“œ: {results['keyword']}")
        print(f"  ì´ ë°ì´í„° ìˆ˜: {results['total_count']:,}ê°œ")
        print(f"  ì‹œê°„ ë²”ìœ„: ìµœê·¼ {results['time_range_hours']}ì‹œê°„")
        print(f"  ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {results['quality_score']:.2%}")
        
        checks = results["checks"]
        
        # í…ìŠ¤íŠ¸ í’ˆì§ˆ
        print(f"\nğŸ“ í…ìŠ¤íŠ¸ í’ˆì§ˆ")
        text_q = checks.get("text_quality", {})
        print(f"  ë¹ˆ í…ìŠ¤íŠ¸ ë¹„ìœ¨: {text_q.get('empty_text_ratio', 0):.2%}")
        print(f"  ì´ëª¨ì§€ í¬í•¨ ë¹„ìœ¨: {text_q.get('emoji_text_ratio', 0):.2%}")
        print(f"  URL í¬í•¨ ë¹„ìœ¨: {text_q.get('url_text_ratio', 0):.2%}")
        print(f"  í‰ê·  ê³µë°± ë¹„ìœ¨: {text_q.get('avg_whitespace_ratio', 0):.2%}")
        print(f"  ì ìˆ˜: {text_q.get('score', 0):.2%}")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„
        print(f"\nâ° íƒ€ì„ìŠ¤íƒ¬í”„")
        timestamp = checks.get("timestamp", {})
        print(f"  NULL íƒ€ì„ìŠ¤íƒ¬í”„ ë¹„ìœ¨: {timestamp.get('null_timestamp_ratio', 0):.2%}")
        print(f"  ë¯¸ë˜ íƒ€ì„ìŠ¤íƒ¬í”„ ë¹„ìœ¨: {timestamp.get('future_timestamp_ratio', 0):.2%}")
        print(f"  ì˜¤ë˜ëœ íƒ€ì„ìŠ¤íƒ¬í”„ ë¹„ìœ¨: {timestamp.get('old_timestamp_ratio', 0):.2%}")
        print(f"  ì ìˆ˜: {timestamp.get('score', 0):.2%}")
        
        # ì–¸ì–´
        print(f"\nğŸŒ ì–¸ì–´ ë¶„í¬")
        language = checks.get("language", {})
        print(f"  í•œêµ­ì–´ ë¹„ìœ¨: {language.get('korean_ratio', 0):.2%}")
        print(f"  ì˜ì–´ ë¹„ìœ¨: {language.get('english_ratio', 0):.2%}")
        print(f"  í˜¼í•© ë¹„ìœ¨: {language.get('mixed_ratio', 0):.2%}")
        print(f"  ì ìˆ˜: {language.get('score', 0):.2%}")
        
        # ì¤‘ë³µ
        print(f"\nğŸ”„ ì¤‘ë³µ ê²€ì‚¬")
        duplicates = checks.get("duplicates", {})
        print(f"  ì´ ê°œìˆ˜: {duplicates.get('total_count', 0):,}ê°œ")
        print(f"  ê³ ìœ  ê°œìˆ˜: {duplicates.get('unique_count', 0):,}ê°œ")
        print(f"  ì¤‘ë³µ ê°œìˆ˜: {duplicates.get('duplicate_count', 0):,}ê°œ")
        print(f"  ì¤‘ë³µ ë¹„ìœ¨: {duplicates.get('duplicate_ratio', 0):.2%}")
        print(f"  ì ìˆ˜: {duplicates.get('score', 0):.2%}")
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        if "keyword_match" in checks:
            print(f"\nğŸ” í‚¤ì›Œë“œ ë§¤ì¹­")
            keyword_match = checks["keyword_match"]
            print(f"  ë§¤ì¹­ëœ ê°œìˆ˜: {keyword_match.get('matched_count', 0):,}ê°œ")
            print(f"  ë§¤ì¹­ ë¹„ìœ¨: {keyword_match.get('match_ratio', 0):.2%}")
            print(f"  ì ìˆ˜: {keyword_match.get('score', 0):.2%}")
        
        print("\n" + "=" * 70)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--keyword", type=str, help="ê²€ì¦í•  í‚¤ì›Œë“œ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´)")
    parser.add_argument("--hours", type=int, default=24, help="ê²€ì¦í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„, ê¸°ë³¸ê°’: 24)")
    
    args = parser.parse_args()
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    init_database("sqlite:///data/database/sentiment.db")
    db = next(get_db())
    
    try:
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        checker = DataQualityChecker(db)
        results = checker.check_all(args.keyword, args.hours)
        
        # ë¦¬í¬íŠ¸ ì¶œë ¥
        checker.print_report(results)
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        db.close()


if __name__ == "__main__":
    main()

