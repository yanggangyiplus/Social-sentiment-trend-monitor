"""
Streamlit ê¸°ë°˜ ì‹¤ì‹œê°„ ê°ì • ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
í‚¤ì›Œë“œ ê²€ìƒ‰ ë° ì†ŒìŠ¤ë³„ ë°ì´í„° í‘œì‹œ
"""
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
from datetime import datetime, timedelta
from pathlib import Path
import sys
import time
import threading
from collections import defaultdict
import io
import re
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.database.db_manager import init_database, get_db
from src.database.models import SentimentAnalysis, CollectedText
from src.trend.trend_utils import TrendAnalyzer
from src.trend.simple_change_detector import SimpleChangeDetector
from src.collectors.collector_manager import CollectorManager
from src.sentiment.sentiment_utils import SentimentAnalyzer
from src.preprocessing.text_cleaner import TextCleaner

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Social Sentiment & Trend Monitor",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
init_database("sqlite:///data/database/sentiment.db")


@st.cache_data(ttl=5)  # ìºì‹œ ì‹œê°„ ë‹¨ì¶• (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)
def get_sentiment_data(keyword: str, source: str, hours: int = 24):
    """ê°ì • ë¶„ì„ ë°ì´í„° ì¡°íšŒ"""
    db = next(get_db())
    try:
        start_time = datetime.utcnow() - timedelta(hours=hours)
        sentiments = db.query(SentimentAnalysis).filter(
            SentimentAnalysis.keyword == keyword,
            SentimentAnalysis.source == source,
            SentimentAnalysis.analyzed_at >= start_time
        ).order_by(SentimentAnalysis.analyzed_at).all()
        
        data = []
        for s in sentiments:
            data.append({
                "analyzed_at": s.analyzed_at,
                "positive_score": s.positive_score,
                "negative_score": s.negative_score,
                "neutral_score": s.neutral_score,
                "predicted_sentiment": s.predicted_sentiment,
                "text_id": s.text_id
            })
        
        return data
    finally:
        db.close()


@st.cache_data(ttl=5)  # ìºì‹œ ì‹œê°„ ë‹¨ì¶• (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)
def get_video_data(keyword: str):
    """YouTube ë¹„ë””ì˜¤ ì •ë³´ ì¡°íšŒ"""
    db = next(get_db())
    try:
        videos = db.query(CollectedText).filter(
            CollectedText.keyword == keyword,
            CollectedText.source == "youtube",
            CollectedText.video_id.isnot(None)
        ).distinct(CollectedText.video_id).all()
        
        video_dict = {}
        for video in videos:
            video_id = video.video_id
            if video_id and video_id not in video_dict:
                video_dict[video_id] = {
                    "video_id": video_id,
                    "title": video.video_title or "ì œëª© ì—†ìŒ",
                    "channel_name": video.channel_name or "ì±„ë„ëª… ì—†ìŒ",
                    "view_count": video.view_count or 0,
                    "like_count": video.like_count or 0,
                    "url": video.url or f"https://www.youtube.com/watch?v={video_id}"
                }
        
        return list(video_dict.values())
    finally:
        db.close()


def run_data_collection(keyword: str, sources: list, max_results: int = 10):
    """ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
    try:
        collector_manager = CollectorManager()
        
        # ì„ íƒëœ ì†ŒìŠ¤ë§Œ í™œì„±í™”
        enabled_sources = []
        if "youtube" in sources:
            enabled_sources.append("youtube")
        if "twitter" in sources:
            enabled_sources.append("twitter")
        if "news" in sources:
            enabled_sources.append("news")
        if "blog" in sources:
            enabled_sources.append("blog")
        
        if not enabled_sources:
            return False, "ì†ŒìŠ¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
        
        collected_data = collector_manager.collect_all(
            keyword, 
            max_results, 
            save_to_database=True
        )
        return True, len(collected_data)
    except Exception as e:
        return False, str(e)


def run_sentiment_analysis(keyword: str, source: str, hours: int = 24):
    """ê°ì • ë¶„ì„ ì‹¤í–‰"""
    try:
        db = next(get_db())
        from datetime import datetime, timedelta
        
        start_time = datetime.utcnow() - timedelta(hours=hours)
        
        # ì´ë¯¸ ë¶„ì„ëœ í…ìŠ¤íŠ¸ ID ì¡°íšŒ
        analyzed_text_ids = db.query(SentimentAnalysis.text_id).filter(
            SentimentAnalysis.keyword == keyword,
            SentimentAnalysis.source == source
        ).subquery()
        
        # ë¶„ì„í•  í…ìŠ¤íŠ¸ ì¡°íšŒ
        texts_to_analyze = db.query(CollectedText).filter(
            CollectedText.keyword == keyword,
            CollectedText.source == source,
            CollectedText.collected_at >= start_time,
            ~CollectedText.id.in_(analyzed_text_ids)
        ).all()
        
        if not texts_to_analyze:
            db.close()
            return True, 0
        
        # ê°ì • ë¶„ì„ ìˆ˜í–‰
        sentiment_analyzer = SentimentAnalyzer()
        text_cleaner = TextCleaner()
        
        analyzed_count = 0
        for text_obj in texts_to_analyze:
            try:
                cleaned_text = text_cleaner.clean_text_for_sentiment(text_obj.text)
                if not cleaned_text or len(cleaned_text.strip()) < 5:
                    continue
                
                result = sentiment_analyzer.analyze(cleaned_text)
                
                sentiment_obj = SentimentAnalysis(
                    text_id=text_obj.id,
                    keyword=text_obj.keyword,
                    source=text_obj.source,
                    positive_score=result['positive_score'],
                    negative_score=result['negative_score'],
                    neutral_score=result['neutral_score'],
                    predicted_sentiment=result['predicted_sentiment'],
                    model_type=result.get('model_type', 'unknown'),
                    analyzed_at=datetime.utcnow()
                )
                db.add(sentiment_obj)
                analyzed_count += 1
            except Exception:
                continue
        
        db.commit()
        db.close()
        return True, analyzed_count
    except Exception as e:
        if db:
            db.close()
        return False, str(e)


def calculate_sentiment_score(positive: float, negative: float, neutral: float) -> float:
    """ê°ì • ì ìˆ˜ ê³„ì‚° (-1 ~ 1)"""
    return positive * 1.0 + neutral * 0.0 + negative * (-1.0)


def format_number(num: int) -> str:
    """ìˆ«ì í¬ë§·íŒ… (ì˜ˆ: 1000 -> 1K)"""
    if num >= 1000000:
        return f"{num/1000000:.1f}M"
    elif num >= 1000:
        return f"{num/1000:.1f}K"
    return str(num)


def generate_wordcloud(texts: list, sentiment_type: str = "all") -> np.ndarray:
    """Word Cloud ìƒì„± (í•œêµ­ì–´ ì§€ì›)"""
    try:
        import platform
        import os
        
        # í…ìŠ¤íŠ¸ ê²°í•©
        combined_text = " ".join(texts)
        
        if not combined_text or len(combined_text.strip()) < 10:
            return None
        
        # í•œêµ­ì–´ í°íŠ¸ ê²½ë¡œ ì°¾ê¸°
        font_path = None
        if platform.system() == 'Darwin':  # macOS
            font_paths = [
                '/System/Library/Fonts/Supplemental/AppleGothic.ttf',
                '/System/Library/Fonts/AppleGothic.ttf',
                '/Library/Fonts/AppleGothic.ttf',
            ]
            for path in font_paths:
                if os.path.exists(path):
                    font_path = path
                    break
        elif platform.system() == 'Windows':  # Windows
            font_paths = [
                'C:/Windows/Fonts/malgun.ttf',  # ë§‘ì€ ê³ ë”•
                'C:/Windows/Fonts/gulim.ttc',    # êµ´ë¦¼
            ]
            for path in font_paths:
                if os.path.exists(path):
                    font_path = path
                    break
        elif platform.system() == 'Linux':  # Linux
            font_paths = [
                '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',
                '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',
            ]
            for path in font_paths:
                if os.path.exists(path):
                    font_path = path
                    break
        
        # Word Cloud ìƒì„±
        if sentiment_type == "positive":
            colors = ['#2ecc71', '#27ae60', '#229954']
        elif sentiment_type == "negative":
            colors = ['#e74c3c', '#c0392b', '#a93226']
        else:
            colors = ['#3498db', '#2980b9', '#1f618d']
        
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            font_path=font_path,  # í•œêµ­ì–´ í°íŠ¸ ê²½ë¡œ ì§€ì •
            max_words=100,
            colormap='viridis' if sentiment_type == "all" else 'Greens' if sentiment_type == "positive" else 'Reds',
            relative_scaling=0.5,
            random_state=42
        ).generate(combined_text)
        
        # ì´ë¯¸ì§€ë¡œ ë³€í™˜
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        
        # Streamlitìš© ì´ë¯¸ì§€ ë³€í™˜
        img_buffer = io.BytesIO()
        plt.savefig(img_buffer, format='png', bbox_inches='tight', dpi=150)
        img_buffer.seek(0)
        plt.close()
        
        return img_buffer
    except Exception as e:
        print(f"Word Cloud ìƒì„± ì‹¤íŒ¨: {e}")
        return None


def auto_collect_and_analyze(keyword: str, sources: list, interval_minutes: int = 5):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìë™ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„"""
    try:
        # ë°ì´í„° ìˆ˜ì§‘
        collector_manager = CollectorManager()
        collected_data = collector_manager.collect_all(
            keyword, 
            10,  # ì†ŒëŸ‰ë§Œ ìˆ˜ì§‘
            save_to_database=True
        )
        
        # ê°ì • ë¶„ì„ (YouTubeë§Œ)
        if "youtube" in sources and collected_data:
            run_sentiment_analysis(keyword, "youtube", 24)
        
        return True, len(collected_data)
    except Exception as e:
        return False, str(e)


def main():
    """ë©”ì¸ ëŒ€ì‹œë³´ë“œ í•¨ìˆ˜"""
    st.title("ğŸ“Š Social Sentiment & Trend Monitor")
    
    # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ìƒíƒœ í‘œì‹œ
    if 'realtime_monitoring' not in st.session_state:
        st.session_state.realtime_monitoring = False
    if 'last_update_time' not in st.session_state:
        st.session_state.last_update_time = None
    if 'monitoring_keyword' not in st.session_state:
        st.session_state.monitoring_keyword = None
    if 'monitoring_sources' not in st.session_state:
        st.session_state.monitoring_sources = []
    
    # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ìƒíƒœ í‘œì‹œ
    if st.session_state.realtime_monitoring:
        status_container = st.container()
        with status_container:
            col1, col2, col3 = st.columns([2, 2, 1])
            with col1:
                st.markdown("ğŸŸ¢ **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ í™œì„±í™”**")
            with col2:
                if st.session_state.last_update_time:
                    elapsed = (datetime.now() - st.session_state.last_update_time).total_seconds()
                    elapsed_minutes = int(elapsed // 60)
                    elapsed_seconds = int(elapsed % 60)
                    if elapsed_minutes > 0:
                        st.markdown(f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {elapsed_minutes}ë¶„ {elapsed_seconds}ì´ˆ ì „")
                    else:
                        st.markdown(f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {elapsed_seconds}ì´ˆ ì „")
            with col3:
                if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", key="refresh_main"):
                    st.cache_data.clear()
                    st.rerun()
    
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°”
    st.sidebar.title("ğŸ” ê²€ìƒ‰ ì„¤ì •")
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰
    search_keyword = st.sidebar.text_input(
        "í‚¤ì›Œë“œ ì…ë ¥",
        placeholder="ì˜ˆ: ì˜í™”ë¦¬ë·°, ì•„ì´í°, ë§¥ë¶...",
        key="search_keyword"
    )
    
    # ì†ŒìŠ¤ ì„ íƒ
    st.sidebar.markdown("### ğŸ“± ì •ë³´ ì†ŒìŠ¤ ì„ íƒ")
    source_youtube = st.sidebar.checkbox("YouTube", value=True)
    source_twitter = st.sidebar.checkbox("X (íŠ¸ìœ„í„°)", value=False)
    source_news = st.sidebar.checkbox("ë‰´ìŠ¤", value=False)
    source_blog = st.sidebar.checkbox("ë¸”ë¡œê·¸", value=False)
    
    selected_sources = []
    if source_youtube:
        selected_sources.append("youtube")
    if source_twitter:
        selected_sources.append("twitter")
    if source_news:
        selected_sources.append("news")
    if source_blog:
        selected_sources.append("blog")
    
    # ì¶”í›„ ì¶”ê°€ë  ì„œë¹„ìŠ¤ ì•ˆë‚´
    if source_twitter or source_news or source_blog:
        st.sidebar.info("âš ï¸ X(íŠ¸ìœ„í„°), ë‰´ìŠ¤, ë¸”ë¡œê·¸ëŠ” ì¶”í›„ ì¶”ê°€ë  ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.")
    
    max_results = st.sidebar.number_input(
        "ìˆ˜ì§‘í•  ë°ì´í„° ìˆ˜",
        min_value=5,
        max_value=50,
        value=10,
        step=5
    )
    
    st.sidebar.markdown("---")
    
    # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì„¤ì • (ê²€ìƒ‰ ë²„íŠ¼ ì „ì— ì •ì˜)
    st.sidebar.markdown("### âš¡ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§")
    
    realtime_enabled = st.sidebar.checkbox(
        "ì‹¤ì‹œê°„ ìë™ ìˆ˜ì§‘ í™œì„±í™”",
        value=st.session_state.realtime_monitoring,
        help="í™œì„±í™” ì‹œ ì£¼ê¸°ì ìœ¼ë¡œ ìë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤."
    )
    
    if realtime_enabled != st.session_state.realtime_monitoring:
        st.session_state.realtime_monitoring = realtime_enabled
        if realtime_enabled:
            st.session_state.monitoring_keyword = search_keyword.strip() if search_keyword else None
            st.session_state.monitoring_sources = selected_sources.copy()
            st.session_state.last_update_time = datetime.now()
            st.sidebar.success("âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        else:
            st.sidebar.info("â¸ï¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    st.sidebar.markdown("---")
    
    # ê²€ìƒ‰ ë²„íŠ¼
    if st.sidebar.button("ğŸ” ê²€ìƒ‰ ë° ë¶„ì„", type="primary"):
        if not search_keyword or not search_keyword.strip():
            st.sidebar.error("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        elif not selected_sources:
            st.sidebar.error("ìµœì†Œ í•˜ë‚˜ì˜ ì†ŒìŠ¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            keyword = search_keyword.strip()
            st.sidebar.info(f"'{keyword}' í‚¤ì›Œë“œë¡œ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # ë°ì´í„° ìˆ˜ì§‘
            with st.spinner(f"'{keyword}' ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
                success, result = run_data_collection(keyword, selected_sources, max_results)
                if success:
                    st.sidebar.success(f"âœ… {result}ê°œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
                else:
                    st.sidebar.error(f"âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {result}")
            
            # ê°ì • ë¶„ì„ (YouTubeë§Œ)
            if success and "youtube" in selected_sources:
                with st.spinner(f"'{keyword}' ê°ì • ë¶„ì„ ì¤‘..."):
                    success2, analyzed_count = run_sentiment_analysis(keyword, "youtube", 24)
                    if success2:
                        if analyzed_count > 0:
                            st.sidebar.success(f"âœ… {analyzed_count}ê°œ í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ")
                        else:
                            st.sidebar.info("â„¹ï¸ ë¶„ì„í•  ìƒˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        st.sidebar.error(f"âŒ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {analyzed_count}")
            
            # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ì„ íƒì‚¬í•­)
            if realtime_enabled:
                st.session_state.monitoring_keyword = keyword
                st.session_state.monitoring_sources = selected_sources.copy()
                st.session_state.last_update_time = datetime.now()
            
            # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
            st.success(f"âœ… '{keyword}' í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ!")
            # ìºì‹œ ì´ˆê¸°í™”
            st.cache_data.clear()
            st.rerun()
    
    hours = st.sidebar.slider("ë¶„ì„ ê¸°ê°„ (ì‹œê°„)", 1, 168, 24)
    
    st.sidebar.markdown("---")
    
    if realtime_enabled:
        interval = st.sidebar.selectbox(
            "ìˆ˜ì§‘ ì£¼ê¸°",
            options=[1, 3, 5, 10, 15, 30],
            index=2,  # ê¸°ë³¸ê°’: 5ë¶„
            format_func=lambda x: f"{x}ë¶„"
        )
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
        if st.session_state.monitoring_keyword and st.session_state.monitoring_sources:
            # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ë¡œë¶€í„° ê²½ê³¼ ì‹œê°„ í™•ì¸
            if st.session_state.last_update_time:
                elapsed_minutes = (datetime.now() - st.session_state.last_update_time).total_seconds() / 60
                if elapsed_minutes >= interval:
                    with st.sidebar.spinner(f"'{st.session_state.monitoring_keyword}' ìë™ ìˆ˜ì§‘ ì¤‘..."):
                        success, result = auto_collect_and_analyze(
                            st.session_state.monitoring_keyword,
                            st.session_state.monitoring_sources,
                            interval
                        )
                        if success:
                            st.session_state.last_update_time = datetime.now()
                            st.sidebar.success(f"âœ… {result}ê°œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
                            # ìºì‹œ ì´ˆê¸°í™”
                            st.cache_data.clear()
                            st.rerun()
                        else:
                            st.sidebar.error(f"âŒ ìˆ˜ì§‘ ì‹¤íŒ¨: {result}")
            else:
                st.session_state.last_update_time = datetime.now()
        
        # ìë™ ìƒˆë¡œê³ ì¹¨ ì•ˆë‚´
        st.sidebar.markdown("---")
        st.sidebar.info("ğŸ’¡ **íŒ:** ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ë³´ë ¤ë©´ í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš” (F5 ë˜ëŠ” Cmd+R)")
        
        # ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼
        if st.sidebar.button("ğŸ”„ ì§€ê¸ˆ ìƒˆë¡œê³ ì¹¨"):
            st.cache_data.clear()
            st.rerun()
    
    # ì„¸ì…˜ ìƒíƒœì— ê²€ìƒ‰ í‚¤ì›Œë“œ ì €ì¥
    if 'current_keyword' not in st.session_state:
        st.session_state.current_keyword = None
    if 'current_source' not in st.session_state:
        st.session_state.current_source = "youtube"
    
    # ë©”ì¸ í™”ë©´
    if not search_keyword or not search_keyword.strip():
        st.warning("ğŸ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³  ê²€ìƒ‰í•´ì£¼ì„¸ìš”.")
        st.info("""
        **ì‚¬ìš© ë°©ë²•:**
        1. ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”
        2. ì •ë³´ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš” (í˜„ì¬ YouTubeë§Œ ì§€ì›)
        3. 'ê²€ìƒ‰ ë° ë¶„ì„' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
        4. ë°ì´í„° ìˆ˜ì§‘ ë° ê°ì • ë¶„ì„ì´ ìë™ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤
        """)
        return
    
    keyword = search_keyword.strip()
    
    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥
    st.markdown("---")
    col1, col2, col3 = st.columns([2, 2, 2])
    
    with col1:
        # ì›ë³¸ ëŒ“ê¸€ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        db = next(get_db())
        try:
            comments_data = db.query(CollectedText).filter(
                CollectedText.keyword == keyword
            ).all()
            
            if comments_data:
                comments_df = pd.DataFrame([{
                    "í‚¤ì›Œë“œ": c.keyword,
                    "ì†ŒìŠ¤": c.source,
                    "ëŒ“ê¸€": c.text,
                    "ì‘ì„±ì": c.author or "",
                    "URL": c.url or "",
                    "ìˆ˜ì§‘ì¼ì‹œ": c.collected_at.strftime("%Y-%m-%d %H:%M:%S") if c.collected_at else "",
                    "ì˜ìƒì œëª©": c.video_title or "",
                    "ì±„ë„ëª…": c.channel_name or "",
                    "ì¡°íšŒìˆ˜": c.view_count or 0,
                    "ì¢‹ì•„ìš”": c.like_count or 0
                } for c in comments_data])
                
                csv_buffer = io.StringIO()
                comments_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
                csv_data = csv_buffer.getvalue()
                
                st.download_button(
                    label="ğŸ“¥ ì›ë³¸ ëŒ“ê¸€ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)",
                    data=csv_data.encode('utf-8-sig'),
                    file_name=f"{keyword}_comments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    key="download_comments"
                )
        finally:
            db.close()
    
    with col2:
        # ê°ì • ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
        db = next(get_db())
        try:
            sentiments_data = db.query(SentimentAnalysis).filter(
                SentimentAnalysis.keyword == keyword
            ).all()
            
            if sentiments_data:
                # ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ë§¤ì¹­
                sentiment_list = []
                for sent in sentiments_data:
                    text_obj = db.query(CollectedText).filter(CollectedText.id == sent.text_id).first()
                    sentiment_list.append({
                        "í‚¤ì›Œë“œ": sent.keyword,
                        "ì†ŒìŠ¤": sent.source,
                        "ëŒ“ê¸€": text_obj.text if text_obj else "",
                        "ì‘ì„±ì": text_obj.author if text_obj else "",
                        "ê¸ì •ì ìˆ˜": f"{sent.positive_score:.4f}",
                        "ë¶€ì •ì ìˆ˜": f"{sent.negative_score:.4f}",
                        "ì¤‘ë¦½ì ìˆ˜": f"{sent.neutral_score:.4f}",
                        "ì˜ˆì¸¡ê°ì •": sent.predicted_sentiment,
                        "ëª¨ë¸íƒ€ì…": sent.model_type,
                        "ë¶„ì„ì¼ì‹œ": sent.analyzed_at.strftime("%Y-%m-%d %H:%M:%S") if sent.analyzed_at else ""
                    })
                
                sentiments_df = pd.DataFrame(sentiment_list)
                
                csv_buffer = io.StringIO()
                sentiments_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
                csv_data = csv_buffer.getvalue()
                
                st.download_button(
                    label="ğŸ“Š ê°ì • ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (CSV)",
                    data=csv_data.encode('utf-8-sig'),
                    file_name=f"{keyword}_sentiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    key="download_sentiment"
                )
        finally:
            db.close()
    
    with col3:
        # í†µê³„ ìš”ì•½ ë‹¤ìš´ë¡œë“œ
        db = next(get_db())
        try:
            sentiments_data = db.query(SentimentAnalysis).filter(
                SentimentAnalysis.keyword == keyword
            ).all()
            
            if sentiments_data:
                sentiment_counts = defaultdict(int)
                total_positive = 0
                total_negative = 0
                total_neutral = 0
                
                for sent in sentiments_data:
                    sentiment_counts[sent.predicted_sentiment] += 1
                    total_positive += sent.positive_score
                    total_negative += sent.negative_score
                    total_neutral += sent.neutral_score
                
                avg_positive = total_positive / len(sentiments_data) if sentiments_data else 0
                avg_negative = total_negative / len(sentiments_data) if sentiments_data else 0
                avg_neutral = total_neutral / len(sentiments_data) if sentiments_data else 0
                overall_sentiment = avg_positive - avg_negative
                
                summary_df = pd.DataFrame([{
                    "í‚¤ì›Œë“œ": keyword,
                    "ì´ëŒ“ê¸€ìˆ˜": len(sentiments_data),
                    "ê¸ì •ê°œìˆ˜": sentiment_counts.get("positive", 0),
                    "ë¶€ì •ê°œìˆ˜": sentiment_counts.get("negative", 0),
                    "ì¤‘ë¦½ê°œìˆ˜": sentiment_counts.get("neutral", 0),
                    "í‰ê· ê¸ì •ì ìˆ˜": f"{avg_positive:.4f}",
                    "í‰ê· ë¶€ì •ì ìˆ˜": f"{avg_negative:.4f}",
                    "í‰ê· ì¤‘ë¦½ì ìˆ˜": f"{avg_neutral:.4f}",
                    "ì „ì²´ê°ì •ìŠ¤ì½”ì–´": f"{overall_sentiment:.4f}",
                    "ìƒì„±ì¼ì‹œ": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }])
                
                csv_buffer = io.StringIO()
                summary_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
                csv_data = csv_buffer.getvalue()
                
                st.download_button(
                    label="ğŸ“ˆ í†µê³„ ìš”ì•½ ë‹¤ìš´ë¡œë“œ (CSV)",
                    data=csv_data.encode('utf-8-sig'),
                    file_name=f"{keyword}_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    key="download_summary"
                )
        finally:
            db.close()
    
    st.markdown("---")
    
    # ì „ì²´ íŠ¸ë Œë“œ ì‹œê°í™” (ë³€í™”ì  Highlight)
    st.header(f"ğŸ“ˆ ì „ì²´ íŠ¸ë Œë“œ ë¶„ì„: '{keyword}'")
    
    db = next(get_db())
    try:
        # ì „ì²´ ê°ì • ë¶„ì„ ë°ì´í„° ì¡°íšŒ
        all_sentiments = db.query(SentimentAnalysis).filter(
            SentimentAnalysis.keyword == keyword,
            SentimentAnalysis.source == "youtube"
        ).order_by(SentimentAnalysis.analyzed_at).all()
        
        if all_sentiments:
            # íŠ¸ë Œë“œ ë¶„ì„ ìˆ˜í–‰
            trend_analyzer = TrendAnalyzer()
            sentiment_list = []
            for sent in all_sentiments:
                sentiment_list.append({
                    "analyzed_at": sent.analyzed_at,
                    "positive_score": sent.positive_score,
                    "negative_score": sent.negative_score,
                    "neutral_score": sent.neutral_score
                })
            
            trend_result = trend_analyzer.analyze_trend(sentiment_list)
            change_points_data = trend_result.get("change_points", [])
            alerts = trend_result.get("alerts", [])
            
            # SimpleChangeDetectorì—ì„œ ë³€í™”ì  ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ìƒì„¸ ì •ë³´ í¬í•¨)
            if isinstance(trend_analyzer.change_detector, SimpleChangeDetector):
                change_points_detail = trend_analyzer.change_detector.detect_changes(sentiment_list)
                # ISO ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                change_points_data = [cp['change_point'] for cp in change_points_detail]
                alerts = change_points_detail
            
            # ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„
            df_trend = pd.DataFrame(sentiment_list)
            df_trend['analyzed_at'] = pd.to_datetime(df_trend['analyzed_at'])
            df_trend['sentiment_score'] = df_trend.apply(
                lambda row: calculate_sentiment_score(
                    row['positive_score'],
                    row['negative_score'],
                    row['neutral_score']
                ),
                axis=1
            )
            
            # ì‹œê°„ë³„ ì§‘ê³„ (1ì‹œê°„ ë‹¨ìœ„)
            df_trend['hour'] = df_trend['analyzed_at'].dt.floor('1H')
            hourly_df = df_trend.groupby('hour').agg({
                'sentiment_score': 'mean',
                'positive_score': 'mean',
                'negative_score': 'mean',
                'neutral_score': 'mean'
            }).reset_index()
            
            # Trend ì„ ê·¸ë˜í”„ + ë³€í™”ì  í‘œì‹œ
            fig_trend = go.Figure()
            
            # ê°ì • ìŠ¤ì½”ì–´ ë¼ì¸
            fig_trend.add_trace(go.Scatter(
                x=hourly_df['hour'],
                y=hourly_df['sentiment_score'],
                mode='lines+markers',
                name='ê°ì • ìŠ¤ì½”ì–´',
                line=dict(color='#3498db', width=3),
                marker=dict(size=8),
                fill='tonexty',
                fillcolor='rgba(52, 152, 219, 0.1)'
            ))
            
            # ë³€í™”ì  Highlight
            if change_points_data:
                for cp in change_points_data:
                    # cpëŠ” ISO í˜•ì‹ ë¬¸ìì—´ì´ê±°ë‚˜ datetime ê°ì²´ì¼ ìˆ˜ ìˆìŒ
                    if isinstance(cp, str):
                        cp_time = pd.to_datetime(cp)
                    elif isinstance(cp, datetime):
                        cp_time = pd.to_datetime(cp)
                    elif isinstance(cp, dict):
                        cp_time_str = cp.get('change_point', '')
                        cp_time = pd.to_datetime(cp_time_str) if cp_time_str else None
                        cp_type = cp.get('change_type', 'unknown')
                        cp_rate = cp.get('change_rate', 0)
                    else:
                        cp_time = pd.to_datetime(cp)
                    
                    if cp_time is None:
                        continue
                    
                    # PlotlyëŠ” datetimeì„ ì§ì ‘ ë°›ì„ ìˆ˜ ìˆì§€ë§Œ, pandas Timestampë¡œ ë³€í™˜
                    cp_time_plotly = pd.to_datetime(cp_time)
                    
                    # ë³€í™”ì ì— ìˆ˜ì§ì„  ì¶”ê°€ (add_shape ì‚¬ìš© - ë” ì•ˆì •ì )
                    fig_trend.add_shape(
                        type="line",
                        x0=cp_time_plotly,
                        x1=cp_time_plotly,
                        y0=hourly_df['sentiment_score'].min() - 0.1,
                        y1=hourly_df['sentiment_score'].max() + 0.1,
                        line=dict(
                            color="red",
                            width=3,
                            dash="dash"
                        ),
                        opacity=0.7
                    )
                    
                    # ë³€í™”ì  ì£¼ì„ ì¶”ê°€
                    fig_trend.add_annotation(
                        x=cp_time_plotly,
                        y=hourly_df['sentiment_score'].max() + 0.05,
                        text="ë³€í™”ì ",
                        showarrow=True,
                        arrowhead=2,
                        arrowcolor="red",
                        font=dict(size=12, color="red"),
                        bgcolor="rgba(255, 255, 255, 0.8)",
                        bordercolor="red",
                        borderwidth=1
                    )
                    # ë³€í™”ì ì— ë§ˆì»¤ ì¶”ê°€ (í•´ë‹¹ ì‹œê°„ì˜ ê°ì • ìŠ¤ì½”ì–´ ì°¾ê¸°)
                    cp_hour = cp_time_plotly.floor('1H')
                    matching_rows = hourly_df[hourly_df['hour'] == cp_hour]
                    if len(matching_rows) > 0:
                        cp_score = matching_rows.iloc[0]['sentiment_score']
                    else:
                        # ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„ ì°¾ê¸°
                        closest_idx = (hourly_df['hour'] - cp_hour).abs().idxmin()
                        cp_score = hourly_df.loc[closest_idx, 'sentiment_score'] if closest_idx < len(hourly_df) else 0
                    
                    fig_trend.add_trace(go.Scatter(
                        x=[cp_time_plotly],
                        y=[cp_score],
                        mode='markers',
                        name='ë³€í™”ì ' if cp == change_points_data[0] else '',
                        marker=dict(
                            size=20,
                            color='red',
                            symbol='diamond',
                            line=dict(width=3, color='darkred')
                        ),
                        showlegend=(cp == change_points_data[0]),
                        hovertemplate=f'ë³€í™”ì <br>ì‹œê°„: {cp_time_plotly}<br>ê°ì • ìŠ¤ì½”ì–´: {cp_score:.3f}<extra></extra>'
                    ))
            
            # ê¸°ì¤€ì„  (0)
            fig_trend.add_hline(
                y=0,
                line_dash="dot",
                line_color="gray",
                line_width=1,
                annotation_text="ì¤‘ë¦½",
                annotation_position="right"
            )
            
            fig_trend.update_layout(
                title=f"ğŸ“ˆ ê°ì • íŠ¸ë Œë“œ íƒ€ì„ë¼ì¸ (ë³€í™”ì  Highlight)",
                xaxis_title="ì‹œê°„",
                yaxis_title="ê°ì • ìŠ¤ì½”ì–´ (-1: ë¶€ì •ì , 0: ì¤‘ë¦½, 1: ê¸ì •ì )",
                hovermode='x unified',
                height=500,
                showlegend=True,
                legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
            )
            
            st.plotly_chart(fig_trend, use_container_width=True, key=f"trend_chart_{keyword}")
            
            # ë³€í™”ì  ìƒì„¸ ì •ë³´
            if alerts:
                st.markdown("---")
                st.markdown("### ğŸš¨ ë³€í™”ì  ìƒì„¸ ì •ë³´")
                alerts_df = pd.DataFrame(alerts)
                
                # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ (SimpleChangeDetectorëŠ” previous_score/current_score ì‚¬ìš©)
                available_columns = []
                column_mapping = {
                    'change_point': 'ë³€í™”ì  ì‹œê°„',
                    'change_type': 'ë³€í™” ìœ í˜•',
                    'change_rate': 'ë³€í™”ìœ¨',
                    'previous_score': 'ì´ì „ ê°ì • ì ìˆ˜',
                    'current_score': 'í˜„ì¬ ê°ì • ì ìˆ˜',
                    'previous_sentiment': 'ì´ì „ ê°ì •',
                    'current_sentiment': 'í˜„ì¬ ê°ì •',
                    'window_start': 'êµ¬ê°„ ì‹œì‘',
                    'window_end': 'êµ¬ê°„ ì¢…ë£Œ'
                }
                
                # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ ì°¾ê¸°
                for col in ['change_point', 'change_type', 'change_rate', 
                           'previous_score', 'current_score', 
                           'previous_sentiment', 'current_sentiment',
                           'window_start', 'window_end']:
                    if col in alerts_df.columns:
                        available_columns.append(col)
                
                if available_columns:
                    display_df = alerts_df[available_columns].copy()
                    # ì»¬ëŸ¼ëª… í•œê¸€ë¡œ ë³€ê²½
                    display_df.columns = [column_mapping.get(col, col) for col in display_df.columns]
                    st.dataframe(
                        display_df,
                        use_container_width=True,
                        hide_index=True
                    )
                else:
                    st.dataframe(alerts_df, use_container_width=True, hide_index=True)
            elif change_points_data:
                st.info(f"âœ… {len(change_points_data)}ê°œì˜ ë³€í™”ì ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.info("íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    finally:
        db.close()
    
    st.markdown("---")
    
    # YouTube ë°ì´í„° í‘œì‹œ
    if "youtube" in selected_sources:
        st.header(f"ğŸ“º YouTube: '{keyword}'")
        
        # ë¹„ë””ì˜¤ ì •ë³´ ì¡°íšŒ
        videos = get_video_data(keyword)
        
        if not videos:
            st.warning(f"í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ YouTube ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ìœ„ì—ì„œ 'ê²€ìƒ‰ ë° ë¶„ì„' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.")
            return
        
        st.markdown(f"**ì´ {len(videos)}ê°œì˜ ì˜ìƒ**")
        st.markdown("---")
        
        # ê° ë¹„ë””ì˜¤ë³„ë¡œ í‘œì‹œ
        for idx, video in enumerate(videos, 1):
            video_id = video["video_id"]
            
            # ë¹„ë””ì˜¤ ì •ë³´ ì¹´ë“œ
            with st.container():
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    st.subheader(f"{idx}. {video['title']}")
                    st.markdown(f"**ì±„ë„:** {video['channel_name']}")
                    st.markdown(f"**URL:** [{video['url']}]({video['url']})")
                
                with col2:
                    st.metric("ì¡°íšŒìˆ˜", format_number(video['view_count']))
                    st.metric("ì¢‹ì•„ìš”", format_number(video['like_count']))
                
                # í•´ë‹¹ ë¹„ë””ì˜¤ì˜ ëŒ“ê¸€ ë° ê°ì • ë¶„ì„ ê²°ê³¼
                db = next(get_db())
                try:
                    # ë¹„ë””ì˜¤ì˜ ëŒ“ê¸€ ì¡°íšŒ
                    comments = db.query(CollectedText).filter(
                        CollectedText.keyword == keyword,
                        CollectedText.video_id == video_id
                    ).order_by(CollectedText.collected_at.desc()).all()
                    
                    # ê°ì • ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ë° ë§¤ì¹­
                    comment_ids = [c.id for c in comments]
                    sentiments_dict = {}
                    if comment_ids:
                        sentiments = db.query(SentimentAnalysis).filter(
                            SentimentAnalysis.text_id.in_(comment_ids)
                        ).all()
                        sentiments_dict = {sent.text_id: sent for sent in sentiments}
                    
                    if sentiments_dict:
                        # ê°ì • í†µê³„
                        sentiment_counts = defaultdict(int)
                        total_positive = 0
                        total_negative = 0
                        total_neutral = 0
                        
                        for sent in sentiments_dict.values():
                            sentiment_counts[sent.predicted_sentiment] += 1
                            total_positive += sent.positive_score
                            total_negative += sent.negative_score
                            total_neutral += sent.neutral_score
                        
                        avg_positive = total_positive / len(sentiments_dict) if sentiments_dict else 0
                        avg_negative = total_negative / len(sentiments_dict) if sentiments_dict else 0
                        avg_neutral = total_neutral / len(sentiments_dict) if sentiments_dict else 0
                        
                        # ì „ì²´ ê°ì • ìŠ¤ì½”ì–´ ê³„ì‚° (-1 ~ 1)
                        overall_sentiment = avg_positive - avg_negative
                        
                        # ìƒìœ„ ëŒ“ê¸€ 5ê°œ í‘œì‹œ
                        st.markdown("---")
                        st.markdown("### ğŸ’¬ ìƒìœ„ ëŒ“ê¸€")
                        
                        # ëŒ“ê¸€ê³¼ ê°ì • ë¶„ì„ ê²°ê³¼ ë§¤ì¹­í•˜ì—¬ ì •ë ¬
                        comment_sentiment_pairs = []
                        for comment in comments[:20]:  # ìµœëŒ€ 20ê°œ ì¤‘ì—ì„œ ì„ íƒ
                            if comment.id in sentiments_dict:
                                sent = sentiments_dict[comment.id]
                                sentiment_score = sent.positive_score - sent.negative_score
                                comment_sentiment_pairs.append((comment, sent, sentiment_score))
                        
                        # ê°ì • ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ê¸ì •/ë¶€ì • ëª¨ë‘ í¬í•¨)
                        comment_sentiment_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
                        top_comments = comment_sentiment_pairs[:5]
                        
                        for i, (comment, sent, score) in enumerate(top_comments, 1):
                            sentiment_label = sent.predicted_sentiment
                            sentiment_emoji = "ğŸ˜Š" if sentiment_label == "positive" else "ğŸ˜¢" if sentiment_label == "negative" else "ğŸ˜"
                            
                            with st.expander(f"{sentiment_emoji} ëŒ“ê¸€ {i}: {comment.text[:50]}..." if len(comment.text) > 50 else f"{sentiment_emoji} ëŒ“ê¸€ {i}: {comment.text}"):
                                st.markdown(f"**ëŒ“ê¸€:** {comment.text}")
                                if comment.author:
                                    st.markdown(f"**ì‘ì„±ì:** {comment.author}")
                                
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("ê¸ì •", f"{sent.positive_score:.2f}", delta=None)
                                with col2:
                                    st.metric("ë¶€ì •", f"{sent.negative_score:.2f}", delta=None)
                                with col3:
                                    st.metric("ì¤‘ë¦½", f"{sent.neutral_score:.2f}", delta=None)
                        
                        st.markdown("---")
                        
                        # ê°ì • ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                        st.markdown("### ğŸ“Š ê°ì • ë¶„ì„ ê²°ê³¼")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("ë¶„ì„ëœ ëŒ“ê¸€", len(sentiments_dict))
                        with col2:
                            st.metric("ê¸ì •", sentiment_counts.get("positive", 0), 
                                    delta=f"{sentiment_counts.get('positive', 0)/len(sentiments_dict)*100:.1f}%")
                        with col3:
                            st.metric("ë¶€ì •", sentiment_counts.get("negative", 0),
                                    delta=f"{sentiment_counts.get('negative', 0)/len(sentiments_dict)*100:.1f}%")
                        with col4:
                            st.metric("ì¤‘ë¦½", sentiment_counts.get("neutral", 0),
                                    delta=f"{sentiment_counts.get('neutral', 0)/len(sentiments_dict)*100:.1f}%")
                        
                        # ì‹œê°ì ì¸ ê·¸ë˜í”„ë“¤
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            # Donut Chart
                            fig_donut = go.Figure(data=[go.Pie(
                                labels=['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½'],
                                values=[sentiment_counts.get("positive", 0), 
                                       sentiment_counts.get("negative", 0),
                                       sentiment_counts.get("neutral", 0)],
                                hole=0.5,
                                marker_colors=['#2ecc71', '#e74c3c', '#95a5a6'],
                                textinfo='label+percent',
                                textposition='outside'
                            )])
                            fig_donut.update_layout(
                                title="ê°ì • ë¶„í¬",
                                height=350,
                                showlegend=True
                            )
                            st.plotly_chart(fig_donut, use_container_width=True, key=f"donut_chart_{video_id}_{idx}")
                        
                        with col2:
                            # Gauge Chart (ì „ì²´ ê°ì • ìŠ¤ì½”ì–´)
                            fig_gauge = go.Figure(go.Indicator(
                                mode="gauge+number+delta",
                                value=overall_sentiment * 100,  # -100 ~ 100 ë²”ìœ„
                                domain={'x': [0, 1], 'y': [0, 1]},
                                title={'text': "ì „ì²´ ê°ì • ìŠ¤ì½”ì–´"},
                                delta={'reference': 0},
                                gauge={
                                    'axis': {'range': [-100, 100]},
                                    'bar': {'color': "#2ecc71" if overall_sentiment > 0 else "#e74c3c" if overall_sentiment < 0 else "#95a5a6"},
                                    'steps': [
                                        {'range': [-100, 0], 'color': "lightgray"},
                                        {'range': [0, 100], 'color': "gray"}
                                    ],
                                    'threshold': {
                                        'line': {'color': "red", 'width': 4},
                                        'thickness': 0.75,
                                        'value': 0
                                    }
                                }
                            ))
                            fig_gauge.update_layout(height=350)
                            st.plotly_chart(fig_gauge, use_container_width=True, key=f"gauge_chart_{video_id}_{idx}")
                        
                        # ê°ì • ì ìˆ˜ ë°” ì°¨íŠ¸ (ê°œì„ )
                        fig_bar = go.Figure()
                        fig_bar.add_trace(go.Bar(
                            name='ê¸ì •',
                            x=['ê°ì • ì ìˆ˜'],
                            y=[avg_positive],
                            marker_color='#2ecc71',
                            text=f'{avg_positive:.2%}',
                            textposition='inside'
                        ))
                        fig_bar.add_trace(go.Bar(
                            name='ë¶€ì •',
                            x=['ê°ì • ì ìˆ˜'],
                            y=[avg_negative],
                            marker_color='#e74c3c',
                            text=f'{avg_negative:.2%}',
                            textposition='inside'
                        ))
                        fig_bar.add_trace(go.Bar(
                            name='ì¤‘ë¦½',
                            x=['ê°ì • ì ìˆ˜'],
                            y=[avg_neutral],
                            marker_color='#95a5a6',
                            text=f'{avg_neutral:.2%}',
                            textposition='inside'
                        ))
                        fig_bar.update_layout(
                            title="í‰ê·  ê°ì • ì ìˆ˜ ë¶„í¬",
                            barmode='stack',
                            height=300,
                            showlegend=True,
                            yaxis=dict(range=[0, 1], title="ë¹„ìœ¨")
                        )
                        st.plotly_chart(fig_bar, use_container_width=True, key=f"bar_chart_{video_id}_{idx}")
                        
                        # Word Cloud ì¶”ê°€
                        st.markdown("---")
                        st.markdown("### â˜ï¸ í‚¤ì›Œë“œ Word Cloud")
                        
                        # ê¸ì •/ë¶€ì • ëŒ“ê¸€ ë¶„ë¦¬
                        positive_texts = []
                        negative_texts = []
                        all_texts = []
                        
                        # ìƒìœ„ ëŒ“ê¸€ì—ì„œ ê¸ì •/ë¶€ì • ë¶„ë¦¬
                        for comment, sent, score in comment_sentiment_pairs:
                            cleaned_text = comment.text
                            sentiment_label = sent.predicted_sentiment.lower()  # ëŒ€ì†Œë¬¸ì í†µì¼
                            
                            if sentiment_label == "positive":
                                positive_texts.append(cleaned_text)
                            elif sentiment_label == "negative":
                                negative_texts.append(cleaned_text)
                            all_texts.append(cleaned_text)
                        
                        # ì „ì²´ ëŒ“ê¸€ì—ì„œë„ ì¶”ê°€ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
                        seen_texts = set(positive_texts + negative_texts)
                        for comment in comments[:100]:  # ë” ë§ì€ ëŒ“ê¸€ í™•ì¸
                            if comment.id in sentiments_dict:
                                sent = sentiments_dict[comment.id]
                                sentiment_label = sent.predicted_sentiment.lower()
                                
                                # ì¤‘ë³µ ì œê±°
                                if comment.text not in seen_texts:
                                    if sentiment_label == "positive":
                                        positive_texts.append(comment.text)
                                        seen_texts.add(comment.text)
                                    elif sentiment_label == "negative":
                                        negative_texts.append(comment.text)
                                        seen_texts.add(comment.text)
                                    all_texts.append(comment.text)
                        
                        # ë””ë²„ê¹… ì •ë³´ (ê°œë°œìš©)
                        # st.write(f"ë””ë²„ê·¸: ê¸ì • {len(positive_texts)}ê°œ, ë¶€ì • {len(negative_texts)}ê°œ")
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown("**ê¸ì • í‚¤ì›Œë“œ**")
                            if positive_texts:
                                st.caption(f"ì´ {len(positive_texts)}ê°œì˜ ê¸ì • ëŒ“ê¸€")
                                wordcloud_img = generate_wordcloud(positive_texts, "positive")
                                if wordcloud_img:
                                    st.image(wordcloud_img, use_container_width=True)
                                else:
                                    st.info("Word Cloud ìƒì„±ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            else:
                                st.info("ê¸ì • ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                                # ë””ë²„ê¹…: ê°ì • ë¶„í¬ í™•ì¸
                                sentiment_dist = {}
                                for comment in comments[:20]:
                                    if comment.id in sentiments_dict:
                                        sent = sentiments_dict[comment.id]
                                        sentiment_dist[sent.predicted_sentiment] = sentiment_dist.get(sent.predicted_sentiment, 0) + 1
                                if sentiment_dist:
                                    st.write(f"ê°ì • ë¶„í¬: {sentiment_dist}")
                        
                        with col2:
                            st.markdown("**ë¶€ì • í‚¤ì›Œë“œ**")
                            if negative_texts:
                                st.caption(f"ì´ {len(negative_texts)}ê°œì˜ ë¶€ì • ëŒ“ê¸€")
                                wordcloud_img = generate_wordcloud(negative_texts, "negative")
                                if wordcloud_img:
                                    st.image(wordcloud_img, use_container_width=True)
                                else:
                                    st.info("Word Cloud ìƒì„±ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            else:
                                st.info("ë¶€ì • ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                        
                        # ë¶„ì„ ì´ìœ  ì„¤ëª…
                        st.markdown("---")
                        st.markdown("### ğŸ’¡ ë¶„ì„ ìš”ì•½")
                        
                        # ê°ì • ë¶„ì„ ê¸°ë°˜ ì„¤ëª… ìƒì„±
                        dominant_sentiment = max(sentiment_counts.items(), key=lambda x: x[1])
                        dominant_ratio = dominant_sentiment[1] / len(sentiments_dict) * 100
                        
                        analysis_text = f"""
                        **ì£¼ìš” ê°ì •:** {dominant_sentiment[0].upper()} ({dominant_ratio:.1f}%)
                        
                        **ë¶„ì„ ê·¼ê±°:**
                        - ì „ì²´ {len(sentiments_dict)}ê°œ ëŒ“ê¸€ ì¤‘ {sentiment_counts.get('positive', 0)}ê°œê°€ ê¸ì •ì , {sentiment_counts.get('negative', 0)}ê°œê°€ ë¶€ì •ì , {sentiment_counts.get('neutral', 0)}ê°œê°€ ì¤‘ë¦½ì ì…ë‹ˆë‹¤.
                        - í‰ê·  ê°ì • ìŠ¤ì½”ì–´ëŠ” {overall_sentiment:.2f}ë¡œ, {'ê¸ì •ì ì¸ ë°˜ì‘ì´ ìš°ì„¸' if overall_sentiment > 0.1 else 'ë¶€ì •ì ì¸ ë°˜ì‘ì´ ìš°ì„¸' if overall_sentiment < -0.1 else 'ì¤‘ë¦½ì ì¸ ë°˜ì‘ì´ ìš°ì„¸'}í•©ë‹ˆë‹¤.
                        - {'ê¸ì •' if avg_positive > avg_negative and avg_positive > avg_neutral else 'ë¶€ì •' if avg_negative > avg_positive and avg_negative > avg_neutral else 'ì¤‘ë¦½'} ê°ì •ì´ ê°€ì¥ ë†’ì€ ë¹„ìœ¨({max(avg_positive, avg_negative, avg_neutral):.1%})ì„ ì°¨ì§€í•©ë‹ˆë‹¤.
                        """
                        
                        st.info(analysis_text)
                    else:
                        st.info("ì´ ì˜ìƒì— ëŒ€í•œ ê°ì • ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                finally:
                    db.close()
                
                st.markdown("---")
    
    # X(íŠ¸ìœ„í„°), ë‰´ìŠ¤, ë¸”ë¡œê·¸ëŠ” ì¶”í›„ ì¶”ê°€ ì•ˆë‚´
    if "twitter" in selected_sources or "news" in selected_sources or "blog" in selected_sources:
        st.markdown("---")
        st.info("""
        **ì¶”í›„ ì¶”ê°€ë  ì„œë¹„ìŠ¤**
        - X (íŠ¸ìœ„í„°): íŠ¸ìœ„í„°/X API ì—°ë™ ì˜ˆì •
        - ë‰´ìŠ¤: ë‰´ìŠ¤ API ì—°ë™ ì˜ˆì •
        - ë¸”ë¡œê·¸: ë¸”ë¡œê·¸ í¬ë¡¤ë§ ê¸°ëŠ¥ ì¶”ê°€ ì˜ˆì •
        
        í˜„ì¬ëŠ” YouTube ë°ì´í„°ë§Œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        """)


if __name__ == "__main__":
    main()
